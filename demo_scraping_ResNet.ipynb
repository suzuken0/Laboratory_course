{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYeIxx0BftgHU5Ogk3VI2h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/Laboratory_course/blob/master/demo_scraping_ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**画像のスクレイピング**\n",
        "\n"
      ],
      "metadata": {
        "id": "25UMQJsL7UPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**iCrawlerを使う方法**\n",
        "\n",
        "https://atmarkit.itmedia.co.jp/ait/articles/2010/28/news018.html\n",
        "\n",
        "公式： https://icrawler.readthedocs.io/en/latest/builtin.html"
      ],
      "metadata": {
        "id": "iMH1ZVOtriop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install icrawler\n",
        "from icrawler.builtin import BingImageCrawler\n",
        "import os\n",
        "\n",
        "# List of keywords\n",
        "keywords = [\"cat\", \"dog\", \"bird\"]\n",
        "max_num = 100\n",
        "\n",
        "for keyword in keywords:\n",
        "    output_dir = f\"/content/{keyword}\"\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    crawler = BingImageCrawler(storage={\"root_dir\": output_dir})\n",
        "    crawler.crawl(keyword=keyword, max_num=max_num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfZC5Uv_r87o",
        "outputId": "338a025a-43ce-460a-d657-81947c04df01"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: icrawler in /usr/local/lib/python3.10/dist-packages (0.6.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from icrawler) (6.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 404, file https://novacatclinic.com/wp-content/uploads/2019/03/IMG_8668.jpg\n",
            "ERROR:downloader:Response status code 403, file https://external-preview.redd.it/i5CeFNrTceC57C_VKC8yT7eXN7iY0M7c4oI9JOJ1nFg.jpg\n",
            "ERROR:downloader:Response status code 403, file https://data.whicdn.com/images/338786605/original.jpg\n",
            "ERROR:downloader:Exception caught when downloading file http://wallpaper.1000webgames.com/wallpapers/russian_blue_cat_wallpaper-1024x768.jpg, error: HTTPConnectionPool(host='wallpaper.1000webgames.com', port=80): Max retries exceeded with url: /wallpapers/russian_blue_cat_wallpaper-1024x768.jpg (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7dde4ffebf40>: Failed to resolve 'wallpaper.1000webgames.com' ([Errno -2] Name or service not known)\")), remaining retry times: 2\n",
            "ERROR:downloader:Exception caught when downloading file http://wallpaper.1000webgames.com/wallpapers/russian_blue_cat_wallpaper-1024x768.jpg, error: HTTPConnectionPool(host='wallpaper.1000webgames.com', port=80): Max retries exceeded with url: /wallpapers/russian_blue_cat_wallpaper-1024x768.jpg (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7dde4fc63ee0>: Failed to resolve 'wallpaper.1000webgames.com' ([Errno -2] Name or service not known)\")), remaining retry times: 1\n",
            "ERROR:downloader:Exception caught when downloading file http://wallpaper.1000webgames.com/wallpapers/russian_blue_cat_wallpaper-1024x768.jpg, error: HTTPConnectionPool(host='wallpaper.1000webgames.com', port=80): Max retries exceeded with url: /wallpapers/russian_blue_cat_wallpaper-1024x768.jpg (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7dde4fcb82b0>: Failed to resolve 'wallpaper.1000webgames.com' ([Errno -2] Name or service not known)\")), remaining retry times: 0\n",
            "ERROR:downloader:Exception caught when downloading file http://wallpaper.1000webgames.com/wallpapers/lhasa_apso_dog_wallpaper-1024x768.jpg, error: HTTPConnectionPool(host='wallpaper.1000webgames.com', port=80): Max retries exceeded with url: /wallpapers/lhasa_apso_dog_wallpaper-1024x768.jpg (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7dde4fb66ad0>: Failed to resolve 'wallpaper.1000webgames.com' ([Errno -2] Name or service not known)\")), remaining retry times: 2\n",
            "ERROR:downloader:Exception caught when downloading file http://wallpaper.1000webgames.com/wallpapers/lhasa_apso_dog_wallpaper-1024x768.jpg, error: HTTPConnectionPool(host='wallpaper.1000webgames.com', port=80): Max retries exceeded with url: /wallpapers/lhasa_apso_dog_wallpaper-1024x768.jpg (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7dde4fb665f0>: Failed to resolve 'wallpaper.1000webgames.com' ([Errno -2] Name or service not known)\")), remaining retry times: 1\n",
            "ERROR:downloader:Exception caught when downloading file http://wallpaper.1000webgames.com/wallpapers/lhasa_apso_dog_wallpaper-1024x768.jpg, error: HTTPConnectionPool(host='wallpaper.1000webgames.com', port=80): Max retries exceeded with url: /wallpapers/lhasa_apso_dog_wallpaper-1024x768.jpg (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7dde4fb662c0>: Failed to resolve 'wallpaper.1000webgames.com' ([Errno -2] Name or service not known)\")), remaining retry times: 0\n",
            "ERROR:downloader:Exception caught when downloading file http://1.bp.blogspot.com/_cUuIMoP_aeE/S73Lj3zCtyI/AAAAAAAACTQ/Z3k9Cyz2OfU/s1600/dog.jpg, error: HTTPConnectionPool(host='1.bp.blogspot.com', port=80): Read timed out. (read timeout=5), remaining retry times: 2\n",
            "ERROR:downloader:Exception caught when downloading file http://1.bp.blogspot.com/_cUuIMoP_aeE/S73Lj3zCtyI/AAAAAAAACTQ/Z3k9Cyz2OfU/s1600/dog.jpg, error: HTTPConnectionPool(host='1.bp.blogspot.com', port=80): Read timed out. (read timeout=5), remaining retry times: 1\n",
            "ERROR:downloader:Response status code 500, file http://1.bp.blogspot.com/_cUuIMoP_aeE/S73Lj3zCtyI/AAAAAAAACTQ/Z3k9Cyz2OfU/s1600/dog.jpg\n",
            "ERROR:downloader:Exception caught when downloading file https://www.myconfinedspace.com/wp-content/uploads/2018/07/crazy-hair-dog-1000x1392.jpg, error: HTTPSConnectionPool(host='www.myconfinedspace.com', port=443): Read timed out. (read timeout=5), remaining retry times: 2\n",
            "ERROR:downloader:Exception caught when downloading file https://www.myconfinedspace.com/wp-content/uploads/2018/07/crazy-hair-dog-1000x1392.jpg, error: HTTPSConnectionPool(host='www.myconfinedspace.com', port=443): Read timed out. (read timeout=5), remaining retry times: 1\n",
            "ERROR:downloader:Exception caught when downloading file https://www.myconfinedspace.com/wp-content/uploads/2018/07/crazy-hair-dog-1000x1392.jpg, error: HTTPSConnectionPool(host='www.myconfinedspace.com', port=443): Read timed out. (read timeout=5), remaining retry times: 0\n",
            "ERROR:downloader:Exception caught when downloading file https://www.hdnicewallpapers.com/Walls/Big/Dog/Cute_Dog_Wallpaper_Collection.jpg, error: HTTPSConnectionPool(host='www.hdnicewallpapers.com', port=443): Max retries exceeded with url: /Walls/Big/Dog/Cute_Dog_Wallpaper_Collection.jpg (Caused by SSLError(SSLCertVerificationError(1, \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'www.hdnicewallpapers.com'. (_ssl.c:1007)\"))), remaining retry times: 2\n",
            "ERROR:downloader:Exception caught when downloading file https://www.hdnicewallpapers.com/Walls/Big/Dog/Cute_Dog_Wallpaper_Collection.jpg, error: HTTPSConnectionPool(host='www.hdnicewallpapers.com', port=443): Max retries exceeded with url: /Walls/Big/Dog/Cute_Dog_Wallpaper_Collection.jpg (Caused by SSLError(SSLCertVerificationError(1, \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'www.hdnicewallpapers.com'. (_ssl.c:1007)\"))), remaining retry times: 1\n",
            "ERROR:downloader:Exception caught when downloading file https://www.hdnicewallpapers.com/Walls/Big/Dog/Cute_Dog_Wallpaper_Collection.jpg, error: HTTPSConnectionPool(host='www.hdnicewallpapers.com', port=443): Max retries exceeded with url: /Walls/Big/Dog/Cute_Dog_Wallpaper_Collection.jpg (Caused by SSLError(SSLCertVerificationError(1, \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'www.hdnicewallpapers.com'. (_ssl.c:1007)\"))), remaining retry times: 0\n",
            "ERROR:downloader:Exception caught when downloading file http://wallpaper.1000webgames.com/wallpapers/funny_pomeranian_dog_wallpaper-1600x1200.jpg, error: HTTPConnectionPool(host='wallpaper.1000webgames.com', port=80): Max retries exceeded with url: /wallpapers/funny_pomeranian_dog_wallpaper-1600x1200.jpg (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7dde4fafd330>: Failed to resolve 'wallpaper.1000webgames.com' ([Errno -2] Name or service not known)\")), remaining retry times: 2\n",
            "ERROR:downloader:Exception caught when downloading file http://wallpaper.1000webgames.com/wallpapers/funny_pomeranian_dog_wallpaper-1600x1200.jpg, error: HTTPConnectionPool(host='wallpaper.1000webgames.com', port=80): Max retries exceeded with url: /wallpapers/funny_pomeranian_dog_wallpaper-1600x1200.jpg (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7dde4fafcd30>: Failed to resolve 'wallpaper.1000webgames.com' ([Errno -2] Name or service not known)\")), remaining retry times: 1\n",
            "ERROR:downloader:Exception caught when downloading file http://wallpaper.1000webgames.com/wallpapers/funny_pomeranian_dog_wallpaper-1600x1200.jpg, error: HTTPConnectionPool(host='wallpaper.1000webgames.com', port=80): Max retries exceeded with url: /wallpapers/funny_pomeranian_dog_wallpaper-1600x1200.jpg (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7dde4fafda80>: Failed to resolve 'wallpaper.1000webgames.com' ([Errno -2] Name or service not known)\")), remaining retry times: 0\n",
            "ERROR:downloader:Exception caught when downloading file https://di7dud5r8j0ls.cloudfront.net/fit-in/1280x853/wordpress/wp-content/uploads/2018/05/kc-reg-coated-mexican-hairless-male-puppy-58d6b6eeb47fa.jpg, error: HTTPSConnectionPool(host='di7dud5r8j0ls.cloudfront.net', port=443): Max retries exceeded with url: /fit-in/1280x853/wordpress/wp-content/uploads/2018/05/kc-reg-coated-mexican-hairless-male-puppy-58d6b6eeb47fa.jpg (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7dde4faff940>: Failed to resolve 'di7dud5r8j0ls.cloudfront.net' ([Errno -5] No address associated with hostname)\")), remaining retry times: 2\n",
            "ERROR:downloader:Exception caught when downloading file https://di7dud5r8j0ls.cloudfront.net/fit-in/1280x853/wordpress/wp-content/uploads/2018/05/kc-reg-coated-mexican-hairless-male-puppy-58d6b6eeb47fa.jpg, error: HTTPSConnectionPool(host='di7dud5r8j0ls.cloudfront.net', port=443): Max retries exceeded with url: /fit-in/1280x853/wordpress/wp-content/uploads/2018/05/kc-reg-coated-mexican-hairless-male-puppy-58d6b6eeb47fa.jpg (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7dde4fb6ba30>: Failed to resolve 'di7dud5r8j0ls.cloudfront.net' ([Errno -5] No address associated with hostname)\")), remaining retry times: 1\n",
            "ERROR:downloader:Exception caught when downloading file https://di7dud5r8j0ls.cloudfront.net/fit-in/1280x853/wordpress/wp-content/uploads/2018/05/kc-reg-coated-mexican-hairless-male-puppy-58d6b6eeb47fa.jpg, error: HTTPSConnectionPool(host='di7dud5r8j0ls.cloudfront.net', port=443): Max retries exceeded with url: /fit-in/1280x853/wordpress/wp-content/uploads/2018/05/kc-reg-coated-mexican-hairless-male-puppy-58d6b6eeb47fa.jpg (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7dde4fb649d0>: Failed to resolve 'di7dud5r8j0ls.cloudfront.net' ([Errno -5] No address associated with hostname)\")), remaining retry times: 0\n",
            "ERROR:downloader:Response status code 404, file https://s3.us-west-1.amazonaws.com/wbforums/monthly_2019_04/518550196_DGC4b.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/97/db/animal_avian_beak_beautiful_bird_birds_birdwatching_blue-1550107.jpg\n",
            "ERROR:downloader:Response status code 403, file http://www.hdnicewallpapers.com/Walls/Big/Eagle/Angry_Bird_Eagle.jpg\n",
            "ERROR:downloader:Exception caught when downloading file https://www.hdnicewallpapers.com/Walls/Big/Crow/bird_Crow_Calling.jpg, error: HTTPSConnectionPool(host='www.hdnicewallpapers.com', port=443): Max retries exceeded with url: /Walls/Big/Crow/bird_Crow_Calling.jpg (Caused by SSLError(SSLCertVerificationError(1, \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'www.hdnicewallpapers.com'. (_ssl.c:1007)\"))), remaining retry times: 2\n",
            "ERROR:downloader:Exception caught when downloading file https://www.hdnicewallpapers.com/Walls/Big/Crow/bird_Crow_Calling.jpg, error: HTTPSConnectionPool(host='www.hdnicewallpapers.com', port=443): Max retries exceeded with url: /Walls/Big/Crow/bird_Crow_Calling.jpg (Caused by SSLError(SSLCertVerificationError(1, \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'www.hdnicewallpapers.com'. (_ssl.c:1007)\"))), remaining retry times: 1\n",
            "ERROR:downloader:Exception caught when downloading file https://www.hdnicewallpapers.com/Walls/Big/Crow/bird_Crow_Calling.jpg, error: HTTPSConnectionPool(host='www.hdnicewallpapers.com', port=443): Max retries exceeded with url: /Walls/Big/Crow/bird_Crow_Calling.jpg (Caused by SSLError(SSLCertVerificationError(1, \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'www.hdnicewallpapers.com'. (_ssl.c:1007)\"))), remaining retry times: 0\n",
            "ERROR:downloader:Response status code 404, file http://newwallpapers1.com/wp-content/uploads/2015/02/Himalayan-Monal-Nepal-National-Bird-3.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TTWvbDZ5r9F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**AZURE Bing Searchを用いる方法**\n",
        "\n",
        "準備：\n",
        "- Microsoft AZUREに登録\n",
        "\n",
        "    https://learn.microsoft.com/ja-jp/azure/cognitive-services/bing-web-search/\n",
        "\n",
        "- 左のタブ → リソースの作成 → Bing Search v7を取得\n",
        "\n",
        "- ダッシュボード → キーとエンドポイントからキーを取得する"
      ],
      "metadata": {
        "id": "539gH-qvrOrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Deep_learning/api.txt\") as file:\n",
        "    #text = file.read()\n",
        "    i=1\n",
        "    key = []\n",
        "    while True:\n",
        "        include_break_line = file.readline() #改行が含まれた行\n",
        "        line = include_break_line.rstrip() #改行を取り除く\n",
        "        if line: #keyの読み込み\n",
        "            #print(f'{i}行目：{line}')\n",
        "            key.append(line)\n",
        "            i += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "bing_api_key = key[13]"
      ],
      "metadata": {
        "id": "Sa4lK_QK7-Uj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b2e0e2e-896f-4937-bd0a-f785b54eda9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "from requests import exceptions\n",
        "import argparse\n",
        "import requests\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "API_KEY = f\"{bing_api_key}\"\n",
        "MAX_SIZE = 10\n",
        "GROUP_SIZE = 5\n",
        "\n",
        "# 取得したエンドポイントURL\n",
        "URL = \"https://api.bing.microsoft.com/v7.0/images/search\"\n",
        "OUTPUT = '/content/save_dir'\n",
        "\n",
        "if not os.path.isdir(OUTPUT):\n",
        "    os.mkdir(OUTPUT)\n",
        "\n",
        "EXCEPTIONS = set([IOError, FileNotFoundError,\n",
        "    exceptions.RequestException, exceptions.HTTPError,\n",
        "    exceptions.ConnectionError, exceptions.Timeout])\n",
        "\n",
        "search_terms = [\"forest\", \"river\", \"house\"]\n",
        "\n",
        "# set the output csv file name\n",
        "csv_file = \"url_list.csv\"\n",
        "\n",
        "# create the csv file and write the headers\n",
        "with open(csv_file, 'w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['Search term', 'Image URL'])\n",
        "\n",
        "# loop over each search term and download images\n",
        "for term in search_terms:\n",
        "    print(f\"[INFO] searching Bing API for '{term}'\")\n",
        "\n",
        "    # create the directory to save the images for the current search term\n",
        "    output_dir = os.path.join(OUTPUT, term)\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    headers = {\"Ocp-Apim-Subscription-Key\": API_KEY}\n",
        "    params = {\"q\": term, \"offset\": 0, \"count\": GROUP_SIZE, \"imageType\": \"Photo\", \"color\": \"ColorOnly\"}\n",
        "\n",
        "    # make the search\n",
        "    search = requests.get(URL, headers=headers, params=params)\n",
        "    search.raise_for_status()\n",
        "\n",
        "    # grab the results from the search, including the total number of\n",
        "    # estimated results returned by the Bing API\n",
        "    results = search.json()\n",
        "    est_num_results = min(results[\"totalEstimatedMatches\"], MAX_RESULTS)\n",
        "    print(f\"[INFO] {est_num_results} total results for '{term}'\")\n",
        "\n",
        "    # initialize the total number of images downloaded thus far\n",
        "    total = 0\n",
        "\n",
        "    # loop over the estimated number of results in `GROUP_SIZE` groups\n",
        "    for offset in range(0, est_num_results, GROUP_SIZE):\n",
        "        # update the search parameters using the current offset, then\n",
        "        # make the request to fetch the results\n",
        "        params[\"offset\"] = offset\n",
        "        search = requests.get(URL, headers=headers, params=params)\n",
        "        search.raise_for_status()\n",
        "        results = search.json()\n",
        "\n",
        "        # loop over the results\n",
        "        for v in results[\"value\"]:\n",
        "            # try to download the image\n",
        "            try:\n",
        "                # make a request to download the image\n",
        "                print(\"[INFO] fetching: {}\".format(v[\"contentUrl\"]))\n",
        "                r = requests.get(v[\"contentUrl\"], timeout=30)\n",
        "\n",
        "                # build the path to the output image\n",
        "                ext = v[\"contentUrl\"][v[\"contentUrl\"].rfind(\".\"):]\n",
        "                filename = f\"{term}_{str(total).zfill(3)}{ext}\"\n",
        "                output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "                # write the image to disk\n",
        "                with open(output_path, \"wb\") as f:\n",
        "                    f.write(r.content)\n",
        "\n",
        "                # write the URL to the csv file\n",
        "                with open(csv_file, 'a', newline='') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([term, v[\"contentUrl\"]])\n",
        "\n",
        "            # catch any errors that would not unable us to download the\n",
        "            # image\n",
        "            except Exception as e:\n",
        "                print(f\"[INFO] skipping: {v['contentUrl']}\")\n",
        "\n",
        "            # if we have reached the maximum number of images, break out\n",
        "            # of the loop\n",
        "            total += 1\n",
        "            print(f\"{total} images downloaded!\")\n",
        "            if total >= MAX_SIZE:\n",
        "                break\n",
        "\n",
        "        # if we have reached the maximum number of images, break out of\n",
        "        # the loop\n",
        "        if total >= MAX_SIZE:\n",
        "            break\n"
      ],
      "metadata": {
        "id": "c_NgBbvCwOrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Chromedriverを用いる方法**"
      ],
      "metadata": {
        "id": "e0WP5ZQnAfIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium==4.1.0 #新しいバージョンだとエラーが出るので旧バージョンにする"
      ],
      "metadata": {
        "id": "9frhTgD4BYLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BoWBLiRVBne3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# これだとサムネイルしか取得できない\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "# Search query\n",
        "search_query = \"flowers\"\n",
        "\n",
        "# Number of images to download\n",
        "num_images = 10\n",
        "\n",
        "# Create a new folder for the images\n",
        "if not os.path.exists(search_query):\n",
        "    os.makedirs(search_query)\n",
        "\n",
        "# URL to search Google Images\n",
        "url = f\"https://www.google.com/search?q={search_query}&tbm=isch\"\n",
        "\n",
        "# Send GET request\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML using Beautiful Soup\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find all image tags\n",
        "images = soup.find_all('img')\n",
        "\n",
        "# Iterate through the images and download them\n",
        "for i, img in enumerate(images[:num_images]):\n",
        "    url = img['src']\n",
        "    print(i)\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        open(f\"{search_query}/{search_query}_{i}.jpg\", \"wb\").write(response.content)\n",
        "    except:\n",
        "        print(\"download error\")"
      ],
      "metadata": {
        "id": "YRrPYIguIEBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!curl -O https://chromedriver.storage.googleapis.com/110.0.5481.77/chromedriver_linux64.zip #Chromeのバージョンに合ったchromedriverのアドレスを設定\n",
        "!unzip chromedriver_linux64.zip\n",
        "!chmod +x chromedriver\n",
        "!mv chromedriver /usr/local/bin/\n",
        "!pip install selenium\n",
        "\n",
        "from selenium import webdriver\n",
        "\n",
        "# Chromeドライバーの設定\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.add_argument('--disable-gpu')\n",
        "options.add_argument('--disable-browser-side-navigation')\n",
        "\n",
        "# Googleで検索する\n",
        "search_query = 'flowers'\n",
        "url = f\"https://www.google.com/search?q={search_query}&tbm=isch\"\n",
        "browser = webdriver.Chrome('chromedriver',options=options)\n",
        "browser.get(url)\n",
        "\n",
        "\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import base64\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# 画像のURLを取得する\n",
        "soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
        "img_tags = soup.find_all('img', class_='rg_i')\n",
        "\n",
        "\n",
        "urls = []\n",
        "for img in img_tags:\n",
        "    try:\n",
        "        urls.append(img[\"src\"])\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "# 画像をダウンロードする\n",
        "if not os.path.exists(search_query):\n",
        "    os.makedirs(search_query)\n",
        "\n",
        "num_images = 10\n",
        "\n",
        "counter = 0\n",
        "for i in range(num_images):\n",
        "    print(urls[i])\n",
        "    image_data = base64.b64decode(urls[i].split(',')[1])\n",
        "\n",
        "    # バイナリデータをBytesIOオブジェクトに書き込む\n",
        "    image_stream = BytesIO(image_data)\n",
        "\n",
        "    # PILで画像オブジェクトを作成する\n",
        "    image = Image.open(image_stream)\n",
        "    image_format = image.format\n",
        "\n",
        "    # 画像のネーミング\n",
        "    num= \"{:04d}\".format(i)\n",
        "    file_name = f\"{search_query}_{num}\"\n",
        "    new_image_path = f\"{search_query}/{file_name}.{image_format}\"\n",
        "\n",
        "\n",
        "    # Save image to file\n",
        "    image.save(new_image_path)\n"
      ],
      "metadata": {
        "id": "RupG3_zyQ7QT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}