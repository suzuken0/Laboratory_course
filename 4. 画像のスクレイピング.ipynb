{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYT3YKns0GUn+0P09G0cGG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/Laboratory_course/blob/master/4.%20%E7%94%BB%E5%83%8F%E3%81%AE%E3%82%B9%E3%82%AF%E3%83%AC%E3%82%A4%E3%83%94%E3%83%B3%E3%82%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**画像のスクレイピング**\n",
        "\n",
        "準備：\n",
        "- Microsoft AZUREに登録\n",
        "\n",
        "    https://learn.microsoft.com/ja-jp/azure/cognitive-services/bing-web-search/\n",
        "\n",
        "- 左のタブ → リソースの作成 → Bing Search v7を取得\n",
        "\n",
        "- ダッシュボード → キーとエンドポイントからキーを取得する\n"
      ],
      "metadata": {
        "id": "25UMQJsL7UPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# with open(\"/content/drive/MyDrive/Deep_learning/api.txt\") as file:\n",
        "#     #text = file.read()\n",
        "#     i=1\n",
        "#     key = []\n",
        "#     while True:\n",
        "#         include_break_line = file.readline() #改行が含まれた行\n",
        "#         line = include_break_line.rstrip() #改行を取り除く\n",
        "#         if line: #keyの読み込み\n",
        "#             #print(f'{i}行目：{line}')\n",
        "#             key.append(line)\n",
        "#             i += 1\n",
        "#         else:\n",
        "#             break\n",
        "\n",
        "# bing_api_key = key[13]"
      ],
      "metadata": {
        "id": "Sa4lK_QK7-Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from requests import exceptions\n",
        "import argparse\n",
        "import requests\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "API_KEY = f\"{bing_api_key}\"\n",
        "MAX_RESULTS = 20\n",
        "GROUP_SIZE = 20\n",
        "\n",
        "# 取得したエンドポイントURL\n",
        "URL = \"https://api.bing.microsoft.com/v7.0/images/search\"\n",
        "OUTPUT = '/content/save_dir'\n",
        "\n",
        "if not os.path.isdir(OUTPUT):\n",
        "    os.mkdir(OUTPUT)\n",
        "\n",
        "EXCEPTIONS = set([IOError, FileNotFoundError,\n",
        "    exceptions.RequestException, exceptions.HTTPError,\n",
        "    exceptions.ConnectionError, exceptions.Timeout])\n",
        "\n",
        "term = 'cat'\n",
        "headers = {\"Ocp-Apim-Subscription-Key\" : API_KEY}\n",
        "params = {\"q\": term, \"offset\": 0, \"count\": GROUP_SIZE, \"imageType\":\"Photo\", \"color\":\"ColorOnly\"}\n",
        "\n",
        "# make the search\n",
        "print(\"[INFO] searching Bing API for '{}'\".format(term))\n",
        "search = requests.get(URL, headers=headers, params=params)\n",
        "search.raise_for_status()\n",
        "\n",
        "# grab the results from the search, including the total number of\n",
        "# estimated results returned by the Bing API\n",
        "results = search.json()\n",
        "estNumResults = min(results[\"totalEstimatedMatches\"], MAX_RESULTS)\n",
        "print(\"[INFO] {} total results for '{}'\".format(estNumResults,\n",
        "    term))\n",
        "\n",
        "# initialize the total number of images downloaded thus far\n",
        "total = 0\n",
        "\n",
        "# loop over the estimated number of results in `GROUP_SIZE` groups\n",
        "for offset in range(0, estNumResults, GROUP_SIZE):\n",
        "    # update the search parameters using the current offset, then\n",
        "    # make the request to fetch the results\n",
        "    print(\"[INFO] making request for group {}-{} of {}...\".format(\n",
        "        offset, offset + GROUP_SIZE, estNumResults))\n",
        "    params[\"offset\"] = offset\n",
        "    search = requests.get(URL, headers=headers, params=params)\n",
        "    search.raise_for_status()\n",
        "    results = search.json()\n",
        "    print(\"[INFO] saving images for group {}-{} of {}...\".format(\n",
        "        offset, offset + GROUP_SIZE, estNumResults))\n",
        "    # loop over the results\n",
        "    for v in results[\"value\"]:\n",
        "        # try to download the image\n",
        "        try:\n",
        "            # make a request to download the image\n",
        "            print(\"[INFO] fetching: {}\".format(v[\"contentUrl\"]))\n",
        "            r = requests.get(v[\"contentUrl\"], timeout=30)\n",
        "\n",
        "            # build the path to the output image\n",
        "            print\n",
        "            ext = v[\"contentUrl\"][v[\"contentUrl\"].rfind(\".\"):v[\"contentUrl\"].rfind(\"?\") if v[\"contentUrl\"].rfind(\"?\") > 0 else None]\n",
        "            p = os.path.sep.join([OUTPUT, \"{}{}\".format(\n",
        "                str(total).zfill(8), ext)])\n",
        "\n",
        "            # write the image to disk\n",
        "            f = open(p, \"wb\")\n",
        "            f.write(r.content)\n",
        "            f.close()\n",
        "\n",
        "        # catch any errors that would not unable us to download the\n",
        "        # image\n",
        "        except Exception as e:\n",
        "            # check to see if our exception is in our list of\n",
        "            # exceptions to check for\n",
        "            if type(e) in EXCEPTIONS:\n",
        "                print(\"[INFO] skipping: {}\".format(v[\"contentUrl\"]))\n",
        "                continue\n",
        "        # try to load the image from disk\n",
        "        image = cv2.imread(p)\n",
        "\n",
        "        # if the image is `None` then we could not properly load the\n",
        "        # image from disk (so it should be ignored)\n",
        "        if image is None:\n",
        "            print(\"[INFO] deleting: {}\".format(p))\n",
        "            os.remove(p)\n",
        "            continue\n",
        "\n",
        "        # update the counter\n",
        "        total += 1"
      ],
      "metadata": {
        "id": "9870EdVS1uVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RDE-ujYJ2ihu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Chromedriverを用いる方法**"
      ],
      "metadata": {
        "id": "e0WP5ZQnAfIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium==4.1.0 #新しいバージョンだとエラーが出るので旧バージョンにする"
      ],
      "metadata": {
        "id": "9frhTgD4BYLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BoWBLiRVBne3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# これだとサムネイルしか取得できない\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "# Search query\n",
        "search_query = \"flowers\"\n",
        "\n",
        "# Number of images to download\n",
        "num_images = 10\n",
        "\n",
        "# Create a new folder for the images\n",
        "if not os.path.exists(search_query):\n",
        "    os.makedirs(search_query)\n",
        "\n",
        "# URL to search Google Images\n",
        "url = f\"https://www.google.com/search?q={search_query}&tbm=isch\"\n",
        "\n",
        "# Send GET request\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML using Beautiful Soup\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find all image tags\n",
        "images = soup.find_all('img')\n",
        "\n",
        "# Iterate through the images and download them\n",
        "for i, img in enumerate(images[:num_images]):\n",
        "    url = img['src']\n",
        "    print(i)\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        open(f\"{search_query}/{search_query}_{i}.jpg\", \"wb\").write(response.content)\n",
        "    except:\n",
        "        print(\"download error\")"
      ],
      "metadata": {
        "id": "YRrPYIguIEBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!curl -O https://chromedriver.storage.googleapis.com/110.0.5481.77/chromedriver_linux64.zip #Chromeのバージョンに合ったchromedriverのアドレスを設定\n",
        "!unzip chromedriver_linux64.zip\n",
        "!chmod +x chromedriver\n",
        "!mv chromedriver /usr/local/bin/\n",
        "!pip install selenium\n",
        "\n",
        "from selenium import webdriver\n",
        "\n",
        "# Chromeドライバーの設定\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.add_argument('--disable-gpu')\n",
        "options.add_argument('--disable-browser-side-navigation')\n",
        "\n",
        "# Googleで検索する\n",
        "search_query = 'flowers'\n",
        "url = f\"https://www.google.com/search?q={search_query}&tbm=isch\"\n",
        "browser = webdriver.Chrome('chromedriver',options=options)\n",
        "browser.get(url)\n",
        "\n",
        "\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import base64\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# 画像のURLを取得する\n",
        "soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
        "img_tags = soup.find_all('img', class_='rg_i')\n",
        "\n",
        "\n",
        "urls = []\n",
        "for img in img_tags:\n",
        "    try:\n",
        "        urls.append(img[\"src\"])\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "# 画像をダウンロードする\n",
        "if not os.path.exists(search_query):\n",
        "    os.makedirs(search_query)\n",
        "\n",
        "num_images = 10\n",
        "\n",
        "counter = 0\n",
        "for i in range(num_images):\n",
        "    print(urls[i])\n",
        "    image_data = base64.b64decode(urls[i].split(',')[1])\n",
        "\n",
        "    # バイナリデータをBytesIOオブジェクトに書き込む\n",
        "    image_stream = BytesIO(image_data)\n",
        "\n",
        "    # PILで画像オブジェクトを作成する\n",
        "    image = Image.open(image_stream)\n",
        "    image_format = image.format\n",
        "\n",
        "    # 画像のネーミング\n",
        "    num= \"{:04d}\".format(i)\n",
        "    file_name = f\"{search_query}_{num}\"\n",
        "    new_image_path = f\"{search_query}/{file_name}.{image_format}\"\n",
        "\n",
        "\n",
        "    # Save image to file\n",
        "    image.save(new_image_path)\n"
      ],
      "metadata": {
        "id": "RupG3_zyQ7QT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rq_JF8cohjj6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}